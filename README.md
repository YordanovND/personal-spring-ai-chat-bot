# Spring AI Chat with Ollama (Dockerized)

This project is a **Spring Boot** application that integrates with **Ollama** to provide AI-powered chat capabilities using local LLMs such as **Meta-Llama-3-8B-Instruct**.  
Itâ€™s fully containerized using **Docker Compose**, so you can spin up both the application and the Ollama backend in one command.

---

## ğŸ§  Project Overview

The app serves a simple REST API that accepts chat prompts from the user and returns AI-generated responses from the LLM running in Ollama.  
It is designed as a **self-contained environment** â€” no external API keys are required because the AI runs locally inside Docker.

---

## ğŸ› ï¸ Technologies Used

- **Java 21** â€” the core language for the backend.
- **Spring Boot 3** â€” application framework for building and serving APIs.
- **Spring AI** â€” abstraction for integrating AI models into Spring applications.
- **Gradle** â€” build and dependency management.
- **Ollama** â€” lightweight LLM runner for local AI inference.
- **Meta-Llama-3-8B-Instruct** â€” default AI model.
- **Docker & Docker Compose** â€” containerization and service orchestration.


## ğŸš€ Getting Started

### 1ï¸âƒ£ Prerequisites
Make sure you have installed:
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Java 21](https://adoptium.net/) (only if running locally without Docker)
- [Gradle](https://gradle.org/) (only if building locally without Docker)

---

### 2ï¸âƒ£ Start with Docker

```bash
# Build and start the services
docker compose up --build

